# ğŸ§  Neural Network from Scratch â€“ MNIST Digit Classifier

This project implements a simple neural network **from scratch using NumPy and Pandas** to classify handwritten digits from the **MNIST dataset**. It is designed as a learning tool to help you understand the internals of how a feedforward neural network works without relying on high-level machine learning libraries like TensorFlow or PyTorch.

---

## ğŸ” Overview

- Architecture:  
  - Input layer: 784 neurons (28Ã—28 pixels flattened)  
  - Hidden layer: 64 neurons, sigmoid activation  
  - Output layer: 10 neurons, softmax activation
- Loss: Categorical Cross-Entropy
- Optimizer: Mini-batch Gradient Descent

---

## ğŸ—ï¸ Features

- Full forward and backward pass using matrix operations
- Cross-entropy loss with softmax output
- One-hot encoding using `pandas.get_dummies()`
- No deep learning libraries â€” only `NumPy`, `Pandas`, and `Matplotlib`
- Accuracy and loss tracking over epochs
- Easy-to-extend design (add layers, change activations, etc.)

---

## ğŸ“¦ Requirements

Install Python dependencies:

```bash
pip install numpy pandas matplotlib tensorflow
```
Note: We use tensorflow.keras.datasets just to load the MNIST data.ğŸš€ How to Run

---

## ğŸ§  Neural Network Architecture

Input (784) â†’ W1 + b1 â†’ Hidden (64, sigmoid) â†’ W2 + b2 â†’ Output (10, softmax)

---

## ğŸ“Œ Notes

- You can switch to ReLU by changing activation functions.
- Extend to multiple hidden layers by adding more forward/backward logic.
- This is a CPU-based implementation and may be slow on large datasets.

